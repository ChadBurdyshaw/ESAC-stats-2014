{
 "metadata": {
  "name": "",
  "signature": "sha256:f0b99c2b55c32249fdab75cb756b6871128293f2ab43418cc4cc9464e3f45e85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Afternoon Breakout: Do-It-Yourself MCMC\n",
      "\n",
      "The standard Bayesian approach to model fitting involves sampling the posterior, usually via a variant of Markov Chain Monte Carlo (MCMC). Though there are many very sophisticated MCMC samplers out there, the most simple algorithm (Metropolis-Hastings) is rather straightforward to code.\n",
      "\n",
      "Here we'll walk through creating our own Metropolis-Hastings sampler from scratch.\n",
      "\n",
      "(The following is based on material put together by Adrian Price-Whelan)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preliminaries\n",
      "\n",
      "As usual, we start with some imports:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# fig_code is in the repository linked above\n",
      "import fig_code\n",
      "\n",
      "# use seaborn plotting defaults\n",
      "# If this causes an error, you can comment it out.\n",
      "import seaborn as sns\n",
      "sns.set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And load some data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from fig_code import linear_data_sample\n",
      "x, y, dy = linear_data_sample()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise\n",
      "\n",
      "Walk through all the following steps, filling-in the code along the way."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First plot the data to see what we're looking at (Use an ``errorbar()`` plot with the provided data)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're going to "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def model(theta, x):\n",
      "    # the `theta` argument is a list of parameter values, e.g., theta = [m, b] for a line\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "We'll start with the assumption that the data are independent and identically distributed so that the likelihood is simply a product of Gaussians (one big Gaussian). We'll also assume that the uncertainties reported are correct, and that there are no uncertainties on the `x` data. We need to define a function that will evaluate the (ln)likelihood of the data, given a particular choice of your model parameters. A good way to structure this function is as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ln_likelihood(theta, x, y, dy):\n",
      "    # we will pass the parameters (theta) to the model function\n",
      "    # the other arguments are the data\n",
      "    pass "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about priors? Remember your prior only depends on the model parameters, but be careful about what kind of prior you are specifying for each parameter. Do we need to properly normalize the probabilities?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ln_prior(theta):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can define a function that evaluates the (ln)posterior probability, which is just the sum of the ln prior and ln likelihood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ln_posterior(theta, x, y, dy):\n",
      "    return ln_prior(theta) + ln_likelihood(theta, x, y, dy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now write a function to actually run a Metropolis-Hastings MCMC sampler. Ford (2005) includes a great step-by-step walkthrough of the Metropolis-Hastings algorithm, and we'll base our code on that"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_mcmc(ln_posterior, nsteps, ndim, p0, stepsize, args=()):\n",
      "    \n",
      "    # Set up the chain, and initialize it\n",
      "    # (need an array holding the positions at each step\n",
      "    \n",
      "    # Set up an array to hold the probabilities at each step\n",
      "    \n",
      "    # Calculate the probability for the first step\n",
      "    \n",
      "    # Loop for nsteps\n",
      "    for i in np.linspace(1,nsteps-1,nsteps-1):\n",
      "        pass\n",
      "        # Randomly choose new model parameters for the trial state\n",
      "        \n",
      "        # Calculate the probability for the new state\n",
      "        \n",
      "        # Compare it to the probability of the old state\n",
      "        # Using the acceptance probability function\n",
      "        \n",
      "        # Chose a random number u between 0 and 1 to compare with p_accept\n",
      "        \n",
      "        # If p_accept>1 or p_accept>u, accept the step\n",
      "            # Save the position to the chain\n",
      "            \n",
      "            # Save the probability to that array\n",
      "            \n",
      "        # Else, do not accept the step\n",
      "            # Set the position and probability are equal to the last value\n",
      "            \n",
      "    # Return the chain and probabilities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now run the MCMC code on the data provided."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the position of the walker as a function of step number for each of the parameters. Are the chains converged? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make histograms of the samples for each parameter. Should you include all of the samples? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Report to us your constraints on the model parameters -- you have some freedom in interpreting what this means..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}